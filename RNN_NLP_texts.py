# -*- coding: utf-8 -*-
"""Обработка текстов "Ultra PRO"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8o599rMMmOEjBuLk_2ItkMXjFd8RY2J
"""

from google.colab import drive, files #библиотека для работы с файлами
import numpy as np
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import os
import re
import collections

from tensorflow.keras import utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

drive.mount('/content/drive')

df = pd.read_csv (open('/content/drive/MyDrive/neuro/data/sms.csv', errors='replace'))
df = df.drop(['comment_id', 'author', 'date'], axis=1)

df['class'].dtypes
df['class'] = df['class'].astype(int)
df['words_len'] = df["content"].apply(lambda x: len(str(x).split(' ')))

xText = np.array(df['content'])
yAll = np.array(df['class'])

xText = xText[10:]
yAll = yAll[10:]

xText.shape

maxWordsCount = 10000

tokenizer = Tokenizer(
    num_words=maxWordsCount,
    filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff',
    lower=True,
    split=' ',
    oov_token='unknown',
    char_level=False)

tokenizer.fit_on_texts(xText) # "Скармливаем" наши тексты, т.е. даём в обработку методу, который соберет словарь частотности

items = list(tokenizer.word_index.items())
print(items)

trainWordIndexes = tokenizer.texts_to_sequences(xText) 
trainWordIndexes_15 = pad_sequences(trainWordIndexes, maxlen=15).tolist()
xTrain01 = tokenizer.sequences_to_matrix(trainWordIndexes_15) 
yAll = utils.to_categorical(yAll, 2)
xTrain, xTest, yTrain, yTest = train_test_split(xTrain01, yAll, test_size=0.2, shuffle = True)

df

"""**Создание архитектуры нейросети DENSE + BOW**"""

model = Sequential()

model.add(Dense(100, input_dim=maxWordsCount, activation='relu'))
model.add(Dropout(0.25))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

model.compile(optimizer=Adam(0.001),
              loss='categorical_crossentropy',
              metrics='accuracy')

hystory = model.fit(xTrain,
                    yTrain,
                    epochs=25,
                    batch_size=15,
                    validation_data=(xTest, yTest))

df_sample = df.sample(n=10)

xCheck = tokenizer.texts_to_sequences(df_sample['content'])
xCheckPad = pad_sequences(xCheck, maxlen=15).tolist()
xCheck10 = tokenizer.sequences_to_matrix(xCheckPad)
currPred = model.predict(xCheck10)
currOut = np.argmax(currPred, axis=1)

"""**РЕЗУЛЬТАТ РАБОТЫ НЕЙРОСЕТИ**"""

df_sample['Определено нейронкой'] = currOut

df_sample.head(10)

"""**ЧАСТОТНОСТЬ СЛОВ**

Спам:
"""

spam = df[df['class'] == 0].content.tolist()

spam = ','.join(spam).split(' ')

tokenizer.fit_on_texts(spam)

items = list(tokenizer.word_index.items())
print(items)

dct = dict(items)

arr = dict(sorted(dct.items(), key=lambda item: item[1]))

arr = dict(map(lambda x: len(x) > 3, arr.items))
