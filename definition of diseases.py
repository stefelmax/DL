# -*- coding: utf-8 -*-
"""RNN Определение болезней

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n44_BGESiniwIdraUMlvKJwO3uEbLXD0

#Распознование заболеваний

##Подготовка дынных

###Импорт библиотек
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import files # Для работы с файлами 
import numpy as np # Для работы с данными 
import pandas as pd # Для работы с таблицами
import matplotlib.pyplot as plt # Для вывода графиков
import os # Для работы с файлами
import gc
import re
# %matplotlib inline

from tensorflow.keras import utils # Для работы с категориальными данными
from tensorflow.keras.models import Sequential # Полносвязная модель
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation, Conv1D, MaxPool1D # Слои для сети
from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами и преобразования их в последовательности
from tensorflow.keras.preprocessing.sequence import pad_sequences # Метод для работы с последовательностями
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional
from tensorflow.keras.utils import plot_model

from sklearn.preprocessing import LabelEncoder # Метод кодирования тестовых лейблов
from sklearn.model_selection import train_test_split # Для разделения выборки на тестовую и обучающую
from google.colab import drive # Для работы с Google Drive
import time # Импортируем библиотеку time
import seaborn as sns # Импортируем библиотеку seaborn
sns.set_style('darkgrid') # Устанавливаем стиль графиков

from google.colab import drive
drive.mount('/content/drive')

"""###Загрузка данных"""

!rm -R /content/disease #Проверяем, есть ли папка. Если да, удаляем её.

! unzip -q '/content/drive/MyDrive/neuro/data/disease.zip' -d '/content/disease' 
#Указываем путь к архиву на Google Drive, и разархивируем его по указанному пути.
# - q означает, что не будет отображаться статус распаковки.
# Можно зайти в файловый менеджер Гугла, найти файл и скопировать к нему путь.

titles = os.listdir('/content/disease/Болезни')
dir = '/content/disease/Болезни/'

def get_data(dir, titles):
  text = [] 
  desease = []

  for t in titles:
    desease.append(t[:-4]) #добавляем в список название болезни без .txt

    with open(dir + t, 'r') as f:
      text.append(f.read().strip())

  return (text, desease)


trainText, yTrain = get_data(dir, titles)
nClasses = yTrain

"""###Токенайзер"""

maxWordsCount = 500

tokenizer = Tokenizer(
    num_words=maxWordsCount,
    filters='!"#$%&()*+,-–—./…:;<=>?@[\\]^_`{|}~«»\t\n\xa0\ufeff',
    lower=True,
    split=' ',
    oov_token='unknown',
    char_level=False)

tokenizer.fit_on_texts(trainText) # "Скармливаем" наши тексты, т.е. даём в обработку методу, который соберет словарь частотности
items = list(tokenizer.word_index.items()) # Вытаскиваем индексы слов для просмотра
trainWordIndexes = tokenizer.texts_to_sequences(trainText) #переводим слова из нашей выборки в индексы частотности (1-неизвестное, 2 самое популярное, и тд)

print(trainWordIndexes[1])

"""###Сэмплирование"""

def sampling(xTrain, yTrain, xLen, xFrame):
  xSample = []
  ySample = []
  index = 0

  for i, xT in enumerate(xTrain):
    index = 0
    while ((index + xLen) < len(xT)):
      xSample.append(xT[index:index+xLen])
      ySample.append((utils.to_categorical(i, len(xTrain))))
      index += xFrame

  xSample = np.array(xSample)
  ySample = np.array(ySample)

  return (xSample, ySample)

xLen = 15
xFrame = 1

xTrain, yTrain =  sampling(trainWordIndexes, yTrain, xLen, xFrame)   
x_train, x_test, y_train, y_test = train_test_split(xTrain, yTrain, test_size=0.2)
x_test, x_check, y_test, y_check = train_test_split(x_test, y_test, test_size=0.5)

"""### Распознавание тестовой выборки"""

def recognizeDisease(model):
  results = model.predict(x_check)

  arr = {}
  evals = {}
  res = []

  for i in range(len(results)):
    res.append(np.argmax(y_check[i]))
    if np.argmax(results[i]) == np.argmax(y_check[i]):
      arr.setdefault(np.argmax(y_check[i]), 0)    
      arr[np.argmax(y_check[i])] += 1

  for k, v in arr.items():
    evals[nClasses[k]] = round(arr[k] / res.count(k) * 100, 2)
    print(nClasses[k], ' '*(10 - len(nClasses[k])), 'распознан с точностью', evals[nClasses[k]], '%')

"""##Эксперименты

### EMBEDDING + DENSE (SIMPLE)
"""

modelED = Sequential()

modelED.add(Embedding(maxWordsCount, 20, input_length=xLen))
modelED.add(Flatten())
modelED.add(Dense(50, activation='relu'))
modelED.add(Dropout(0.2))
modelED.add(BatchNormalization())
modelED.add(Dense(10, activation='softmax'))

modelED.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

modelED.summary()
plot_model(modelED, dpi=60, show_shapes=True)

history_ED = modelED.fit(x_train, y_train,
                              epochs=15,
                              batch_size=28,
                              validation_data=(x_test, y_test))

# Строим график для отображения динамики ошибки работы модели
plt.figure(figsize = (14, 7))
plt.plot(history_ED.history['loss'], 
         label='Значение ошибки на обучающем наборе')
plt.plot(history_ED.history['val_loss'], 
         label='Значение ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Значение ошибки')
plt.legend()
plt.show()

# Строим график для отображения динамики обучения и точности предсказания сети
plt.figure(figsize = (14, 7))
plt.plot(history_ED.history['accuracy'], 
         label='Доля верных ответов на обучающем наборе')
plt.plot(history_ED.history['val_accuracy'], 
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

recognizeDisease(modelED)